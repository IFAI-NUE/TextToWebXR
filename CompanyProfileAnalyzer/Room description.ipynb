{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "SqlCellData": {
     "variableName$1": "df_sql1"
    }
   },
   "cell_type": "markdown",
   "source": "",
   "id": "9a63bb9064673c59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T09:33:35.086019Z",
     "start_time": "2025-09-09T09:33:35.079918Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "# Room Description Generator (Company-Aligned)\n",
    "\n",
    "This notebook:\n",
    "1. **Loads** the prepared two-column datasets (companies & job postings).\n",
    "2. **Builds** a structured **space description** for each company by calling the **OpenAI API** (step 1).\n",
    "3. **Augments** the description with plausible, company-aligned details that may fit but were **not** present in the extracted answers (step 2).\n",
    "4. **Saves** the final data (step 3).\n",
    "\n",
    "> The output is a JSON structure per company that follows the required schema."
   ],
   "id": "bbc3f5762954fc62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration",
   "id": "f0b2d4bae8cff01a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Input paths (two columns: ID + 'text')\n",
    "COMPANIES_PARQUET = Path(\"companies_two_columns.parquet\")\n",
    "COMPANIES_CSV     = Path(\"companies_two_columns.csv\")\n",
    "\n",
    "JOBS_PARQUET      = Path(\"jobs_two_columns.parquet\")\n",
    "JOBS_CSV          = Path(\"jobs_two_columns.csv\")\n",
    "\n",
    "# Column names\n",
    "COMPANY_ID_COL    = \"prsId\"   # adjust if different\n",
    "COMPANY_TEXT_COL  = \"text\"\n",
    "\n",
    "JOB_ID_COL        = \"jobId\"\n",
    "JOB_TEXT_COL      = \"text\"\n",
    "JOB_COMPANY_ID_COL= \"prsId\"   # foreign key Job -> Company (adjust if needed)\n",
    "\n",
    "# Model configuration\n",
    "OPENAI_MODEL_PRIMARY = \"gpt-4o-mini\"  # for generation\n",
    "OPENAI_MODEL_AUGMENT = \"gpt-4o-mini\"  # for augmentation (can be same)\n",
    "\n",
    "# Inference parameters\n",
    "MAX_INPUT_CHARS_PER_COMPANY = 20000  # cap combined context\n",
    "TEMPERATURE_PRIMARY = 0.3\n",
    "TEMPERATURE_AUGMENT = 0.2\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_JSONL = Path(\"space_descriptions.jsonl\")\n",
    "OUTPUT_PARQUET = Path(\"space_descriptions.parquet\")"
   ],
   "id": "a2d046393bbc524f"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql3"
    }
   },
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "f9d14029d10304a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "def _load_two_col(path_parquet: Path, path_csv: Path, id_col: str, text_col: str) -> pd.DataFrame:\n",
    "    if path_parquet.exists():\n",
    "        df = pd.read_parquet(path_parquet)\n",
    "    elif path_csv.exists():\n",
    "        df = pd.read_csv(path_csv)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Neither {path_parquet} nor {path_csv} was found.\")\n",
    "    if id_col not in df.columns or text_col not in df.columns:\n",
    "        raise KeyError(f\"Expected columns missing: id_col={id_col}, text_col={text_col}. Got: {df.columns.tolist()}\")\n",
    "    return df[[id_col, text_col]].dropna().drop_duplicates()\n",
    "\n",
    "companies = _load_two_col(COMPANIES_PARQUET, COMPANIES_CSV, COMPANY_ID_COL, COMPANY_TEXT_COL)\n",
    "jobs      = _load_two_col(JOBS_PARQUET, JOBS_CSV, JOB_ID_COL, JOB_TEXT_COL)\n",
    "\n",
    "# Ensure we have a join key from jobs -> company\n",
    "if JOB_COMPANY_ID_COL not in jobs.columns:\n",
    "    for cand in [\"prsId\", \"companyId\", \"company_id\", \"employerId\"]:\n",
    "        if cand in jobs.columns:\n",
    "            JOB_COMPANY_ID_COL = cand\n",
    "            break\n",
    "    else:\n",
    "        raise KeyError(f\"Join column for Jobs->Company not found. Please set JOB_COMPANY_ID_COL. Available: {jobs.columns.tolist()}\")\n",
    "\n",
    "display(companies.head())\n",
    "display(jobs.head())"
   ],
   "id": "991bb71995b03d12"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build Company Context (Profiles + Job Postings)",
   "id": "2c3f849aa0f10dca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def truncate_chars(s: str, max_chars: int) -> str:\n",
    "    return s if len(s) <= max_chars else s[:max_chars]\n",
    "\n",
    "company_context = {}\n",
    "for _, crow in companies.iterrows():\n",
    "    cid = crow[COMPANY_ID_COL]\n",
    "    ctext = str(crow[COMPANY_TEXT_COL])\n",
    "    job_texts = [str(j) for j in jobs.loc[jobs[JOB_COMPANY_ID_COL] == cid, JOB_TEXT_COL].tolist()]\n",
    "    parts = []\n",
    "    if ctext:\n",
    "        parts.append(\"# Company Profile\\n\" + ctext.strip())\n",
    "    if job_texts:\n",
    "        parts.append(\"# Job Postings\\n\" + \"\\n\\n---\\n\\n\".join([jt.strip() for jt in job_texts if isinstance(jt, str)]))\n",
    "    joined = \"\\n\\n\".join(parts)\n",
    "    company_context[cid] = truncate_chars(joined, MAX_INPUT_CHARS_PER_COMPANY)\n",
    "\n",
    "len(company_context)"
   ],
   "id": "516d42c0dc9e4982"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql2"
    }
   },
   "cell_type": "markdown",
   "source": "## Schema",
   "id": "b577dcee341ad4f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "SPACE_SCHEMA = {\n",
    "  \"identity_brand\": {\n",
    "    \"legal_name\": \"string\",\n",
    "    \"short_tagline\": \"string (<= 80 chars)\",\n",
    "    \"sector\": \"software|manufacturing|healthcare|mobility|finance|energy|media|research|public\",\n",
    "    \"org_maturity\": \"startup|scaleup|sme|enterprise|ngo|public\",\n",
    "    \"geography_scope\": \"local|regional|global\",\n",
    "    \"palette_hex\": [\"#AABBCC\"],  # 1–3\n",
    "    \"shape_language\": \"angular|rounded|mixed\",\n",
    "    \"material_hint\": \"wood|metal|glass|fabric|mixed|sustainable\",\n",
    "    \"logo_usage\": \"digital_only|freestanding_signage|light_projection|none\"\n",
    "  },\n",
    "  \"value_proposition_map\": [\n",
    "    {\n",
    "      \"type\": \"product|service|platform|solution\",\n",
    "      \"name\": \"string\",\n",
    "      \"audience\": \"customers|devs|partners|public\",\n",
    "      \"proof_tokens\": [\"keyword\"],\n",
    "      \"weight_percent\": 0\n",
    "    }\n",
    "  ],  # 2–6 items (weights sum to 100)\n",
    "  \"culture_profile\": {\n",
    "    \"values_keywords\": [\"kw1\", \"kw2\", \"kw3\"],  # 3–6\n",
    "    \"pace\": \"steady|fast|research|regulated\",\n",
    "    \"collaboration_style\": \"pairing|squad|crossfunctional|individual\",\n",
    "    \"work_mode\": \"on_site|hybrid|remote_first\"\n",
    "  },\n",
    "  \"portfolio_focus\": [\n",
    "    {\n",
    "      \"artifact_type\": \"device|app|dataset|prototype|case_study|patent|award\",\n",
    "      \"display_mode\": \"live_demo|interactive_mock|static_model|video_loop|artifact_on_plinth\",\n",
    "      \"handling\": \"open_touch|supervised|staff_only\",\n",
    "      \"safety_notes\": [\"optional note\"]  # 0–3\n",
    "    }\n",
    "  ],  # 1–5\n",
    "  \"narrative_assets\": {\n",
    "    \"headline\": \"string (<= 80)\",\n",
    "    \"story_snippets\": [\"snippet1\", \"snippet2\"],  # 2–4\n",
    "    \"impact_metrics\": [{\"label\": \"string\", \"value\": \"number|string\", \"unit\": \"string\"}],  # 0–5\n",
    "    \"external_signals\": [{\"type\": \"award|certification|standard|open_source|partnership\", \"label\": \"string\"}]  # 0–5\n",
    "  },\n",
    "  \"interaction_flow\": {\n",
    "    \"modes\": [{\"mode\": \"self_guided|staff_guided|scheduled_talks|open_demo\", \"weight_percent\": 0}],  # 1–4, sum=100\n",
    "    \"visitor_path\": [\"Zone A\", \"Zone B\"]\n",
    "  },\n",
    "  \"zones_layout\": {\n",
    "    \"pattern\": \"islands|loop|gallery|theater|clusters\",\n",
    "    \"zones\": [{\"name\": \"welcome|showcase|hands_on|conversation|media|prototype|coffee\", \"relative_area_percent\": 0}]  # 2–6, sum=100\n",
    "  },\n",
    "  \"anchor_objects\": [\n",
    "    {\"type\": \"kiosk|demo_table|high_table|seating_cluster|device_rig|brochure_stand|mobile_whiteboard|display_plinth|plant_cluster|charging_station|brand_tower|media_totem|product_plinth\",\n",
    "     \"count\": 0, \"mobility\": \"fixed|wheeled|portable\"}\n",
    "  ],  # 2–8\n",
    "  \"circulation_accessibility\": {\n",
    "    \"aisle_min_width_m\": 1.2,\n",
    "    \"entry_points\": \"single|dual\",\n",
    "    \"accessibility_notes\": [\"optional\"]  # 0–3\n",
    "  },\n",
    "  \"ambience\": {\n",
    "    \"lighting_signature\": \"spotlit|diffuse|mixed|dynamic\",\n",
    "    \"acoustic_profile\": \"quiet|moderate_buzz|demo_audio\",\n",
    "    \"music\": \"none|low\",\n",
    "    \"scent\": \"none|brand_neutral\"\n",
    "  }\n",
    "}"
   ],
   "id": "baca09a1e29c0a44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prompt Builders",
   "id": "b44f616fc8309fca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "\n",
    "def build_primary_prompt(company_text: str) -> str:\n",
    "    schema_str = json.dumps(SPACE_SCHEMA, indent=2)\n",
    "    return (\n",
    "        \"You are a spatial experience designer. Based **only** on the following company profile + job postings, \"\n",
    "        \"generate a concise JSON object that fills the fields of the provided schema.\\n\"\n",
    "        \"- Keep it plausible and conservative; do not hallucinate specifics beyond what is reasonably suggested by the text.\\n\"\n",
    "        \"- All arrays must respect required cardinalities and weight sums must equal 100 where specified.\\n\"\n",
    "        \"- Use **English**.\\n\"\n",
    "        \"- Output **only valid JSON** without extra commentary.\\n\\n\"\n",
    "        \"### CONTEXT\\n\" + company_text + \"\\n\\n\"\n",
    "        \"### JSON SCHEMA (example / shape, not a strict JSON Schema)\\n\" + schema_str + \"\\n\\n\"\n",
    "        \"### OUTPUT\\nReturn a single JSON object adhering to the schema.\"\n",
    "    )\n",
    "\n",
    "def build_augment_prompt(existing_json: dict, company_text: str) -> str:\n",
    "    init_str = json.dumps(existing_json, indent=2)\n",
    "    return (\n",
    "        \"You are refining a space description JSON for a company. You will propose **additions** that plausibly fit the \"\n",
    "        \"company **but are not explicitly covered** in the provided context or initial JSON.\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"- Stay consistent with the company's domain and tone.\\n\"\n",
    "        \"- Do not contradict the initial JSON.\\n\"\n",
    "        \"- Keep suggestions compact and realistic.\\n\"\n",
    "        \"- Do not repeat fields already well filled.\\n\"\n",
    "        \"- Return only a JSON with fields **to add or refine**, mirroring keys from the original schema where relevant.\\n\\n\"\n",
    "        \"### CONTEXT\\n\" + company_text + \"\\n\\n\"\n",
    "        \"### INITIAL_JSON\\n\" + init_str + \"\\n\\n\"\n",
    "        \"### OUTPUT\\nReturn a JSON object with only **additional or refined** fields (partial structure allowed). If nothing to add, return {}.\"\n",
    "    )"
   ],
   "id": "2f9cac47ebfa5237"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## OpenAI Helper Functions",
   "id": "c8a121ddb7738dac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os, json, time\n",
    "\n",
    "def _init_openai():\n",
    "    # Try the modern client first; fall back to legacy if needed.\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI()\n",
    "        return (\"responses\", client)\n",
    "    except Exception:\n",
    "        import openai\n",
    "        return (\"chat\", openai)\n",
    "\n",
    "API_KIND, OPENAI_CLIENT = _init_openai()\n",
    "\n",
    "def call_openai_json(prompt: str, model: str, temperature: float = 0.2, max_retries: int = 3) -> dict:\n",
    "    \"\"\"Calls the OpenAI API and returns a parsed JSON object from the response text.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if API_KIND == \"responses\":\n",
    "                out = OPENAI_CLIENT.responses.create(\n",
    "                    model=model,\n",
    "                    input=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=temperature,\n",
    "                    max_output_tokens=2000,\n",
    "                    response_format={\"type\":\"json_object\"},\n",
    "                )\n",
    "                text = out.output_text\n",
    "            else:\n",
    "                openai = OPENAI_CLIENT\n",
    "                resp = openai.ChatCompletion.create(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\":\"user\",\"content\": prompt}],\n",
    "                    temperature=temperature,\n",
    "                )\n",
    "                text = resp.choices[0].message[\"content\"]\n",
    "            return json.loads(text)\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(1.5 * (attempt + 1))\n",
    "                continue\n",
    "            raise RuntimeError(f\"OpenAI call failed after {max_retries} attempts: {e}\")"
   ],
   "id": "ce33db3f34c0d68c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 1 — Generate Space Description per Company",
   "id": "62a6089881c4a528"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "primary_json_by_company = {}\n",
    "\n",
    "for cid, ctx in company_context.items():\n",
    "    prompt = build_primary_prompt(ctx)\n",
    "    primary = call_openai_json(prompt, model=OPENAI_MODEL_PRIMARY, temperature=TEMPERATURE_PRIMARY)\n",
    "    primary_json_by_company[cid] = primary\n",
    "\n",
    "len(primary_json_by_company)"
   ],
   "id": "f0cfe910896ba738"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 2 — Augment with Plausible, Company-Aligned Info",
   "id": "ac64f4224d879178"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def deep_merge(a, b):\n",
    "    if isinstance(a, dict) and isinstance(b, dict):\n",
    "        out = a.copy()\n",
    "        for k, v in b.items():\n",
    "            if k in out:\n",
    "                out[k] = deep_merge(out[k], v)\n",
    "            else:\n",
    "                out[k] = v\n",
    "        return out\n",
    "    return b if b is not None else a\n",
    "\n",
    "augmented_json_by_company = {}\n",
    "\n",
    "for cid, base in primary_json_by_company.items():\n",
    "    ctx = company_context[cid]\n",
    "    prompt = build_augment_prompt(base, ctx)\n",
    "    try:\n",
    "        augment = call_openai_json(prompt, model=OPENAI_MODEL_AUGMENT, temperature=TEMPERATURE_AUGMENT)\n",
    "    except Exception:\n",
    "        augment = {}\n",
    "    merged = deep_merge(base, augment)\n",
    "    augmented_json_by_company[cid] = merged\n",
    "\n",
    "len(augmented_json_by_company)"
   ],
   "id": "cba0c09142623637"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save Results",
   "id": "20ed8a84d9ea3d05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json, pandas as pd\n",
    "\n",
    "# Save JSONL (one JSON per line)\n",
    "with open(OUTPUT_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for cid, js in augmented_json_by_company.items():\n",
    "        rec = {\"company_id\": cid, \"space_description\": js}\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# Also flatten a few top-level fields for a quick Parquet table\n",
    "flat_rows = []\n",
    "for cid, js in augmented_json_by_company.items():\n",
    "    ib = js.get(\"identity_brand\", {}) if isinstance(js, dict) else {}\n",
    "    flat_rows.append({\n",
    "        \"company_id\": cid,\n",
    "        \"legal_name\": ib.get(\"legal_name\"),\n",
    "        \"short_tagline\": ib.get(\"short_tagline\"),\n",
    "        \"sector\": ib.get(\"sector\"),\n",
    "        \"org_maturity\": ib.get(\"org_maturity\"),\n",
    "        \"geography_scope\": ib.get(\"geography_scope\"),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(flat_rows)\n",
    "try:\n",
    "    df.to_parquet(OUTPUT_PARQUET, index=False)\n",
    "except Exception as e:\n",
    "    print(\"Skipped Parquet (install pyarrow):\", e)\n",
    "\n",
    "print(f\"Saved JSONL -> {OUTPUT_JSONL.resolve()}\")\n",
    "try:\n",
    "    print(f\"Saved Parquet -> {OUTPUT_PARQUET.resolve()}\")\n",
    "except Exception:\n",
    "    pass"
   ],
   "id": "9595c358545925c1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
